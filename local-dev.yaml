# BoilStream Development Configuration
# This file contains all configurable settings for local development.
# Environment variables will override these settings.

# AWS Configuration
aws:
  region: "eu-west-1"
  # access_key_id: "your-access-key"  # Optional - can use AWS CLI/IAM roles
  # secret_access_key: "your-secret-key"  # Optional - can use AWS CLI/IAM roles
  https_conn_pool_size: 100

# Storage Configuration
storage:
  # Multiple storage backends can be configured simultaneously
  backends:
    - name: "primary-s3"
      backend_type: "s3"
      enabled: true
      primary: true  # Primary backend - operations must succeed here
      endpoint: "http://localhost:9000"
      access_key: "minioadmin"
      secret_key: "minioadmin"
      region: "us-east-1"
      use_path_style: true  # Required for MinIO
      bucket: "ingestion-data"
      prefix: "/"
      max_concurrent_uploads: 100
      upload_id_pool_capacity: 100
      max_retries: 3
      initial_backoff_ms: 100
      max_retry_attempts: 3
      flush_interval_ms: 10000
      max_multipart_object_size: 104857600  # 100 MB
      # DuckLake integration - register files with these catalogs after successful upload
      ducklake: ["my_ducklake"]
    # - name: "backup-filesystem"
    #   backend_type: "filesystem"
    #   enabled: true
    #   primary: false  # Secondary backend - failures are logged but not fatal
    #   prefix: "/tmp/storage"
    # - name: "debug-noop"
    #   backend_type: "noop"
    #   enabled: false
    #   primary: false  # For testing/benchmarking without actual storage

# Server Configuration
server:
  valkey_url: "redis://localhost:6379" # Schema registry
  # tokio_worker_threads: 16  # Optional - defaults to system CPU count
  flight_thread_count: 1 # One is sufficient for GBs/s throughput
  flight_base_port: 50050 # You basically hit port 50051.
  admin_flight_port: 50160 # Not in use yet.
  consumer_flight_port: 50250 # Not in use yet.
  router:
    # SPSC queue capacity per processing thread (number of TopicBatch objects)
    # Controls memory usage: total buffered = spsc_queue_capacity × num_threads × batch_size
    spsc_queue_capacity: 2  # Low latency for development (2 × 8 × 500 = 8k messages max)
    # Number of individual messages per topic batch
    batch_size: 200 # How much to pick from the queue (max 2048). Keep fairly low for smooth stream.
    # Retry delay when processing threads are busy (milliseconds)
    retry_delay_ms: 1

# Data Processing Configuration
processing:
  data_processing_threads: 8 # Priority based data processing thread selection, max threads
  buffer_pool_max_size: 50
  window_queue_capacity: 30000
  window_ms: 1000
  include_metadata_columns: true
  schema_validation_enabled: true
  parquet:
    compression: "ZSTD"
    dictionary_enabled: true

# DuckDB Topic Persistence Configuration
# Per-topic database files for real-time ingestion persistence
# This is separate from storage backends and Hive partitioned Parquet files
duckdb_persistence:
  enabled: true  # Disable DuckDB persistence completely
  # Directory for topic-specific database files (separate from analytical DuckDB)
  # Each topic gets: {storage_path}/topic_{id}_{name}.duckdb
  storage_path: "/tmp/duckdb/topics"
  max_writers: 10 # Priority based DuckDB database ingestion, max databases

# DuckLake Configuration
ducklake:
  - name: my_ducklake
    # Data path where Parquet files are stored (must match storage backend path)
    # For reconciliation and configuration validation (we don't want to try to parse the SQL to find the data path)
    data_path: "s3://ingestion-data/"
    # See https://ducklake.select/docs/stable/duckdb/usage/connecting#parameters
    attach: |
      FROM duckdb_secrets();
      -- Load extensions
      INSTALL ducklake;
      INSTALL postgres;
      INSTALL aws;
      INSTALL httpfs;
      LOAD ducklake;
      LOAD postgres;
      LOAD aws;
      LOAD httpfs;
      -- S3 access for MinIO
      CREATE OR REPLACE SECRET secretForDirectS3Access (
          TYPE S3,
          KEY_ID 'minioadmin',
          SECRET 'minioadmin',
          REGION 'us-east-1',
          ENDPOINT 'localhost:9000',
          USE_SSL false,
          URL_STYLE 'path',
          SCOPE 's3://ingestion-data/'
      );
      -- PostgreSQL secret for DuckLake catalog backend
      CREATE OR REPLACE SECRET postgres (
        TYPE POSTGRES,
        HOST 'localhost',
        PORT 5432,
        DATABASE 'boilstream',
        USER 'postgres',
        PASSWORD 'postgres'
      );
      -- DuckLake Postgres catalog
      CREATE OR REPLACE SECRET pg_secret (
        TYPE DUCKLAKE,
        METADATA_PATH '',
        DATA_PATH 's3://ingestion-data/',
        METADATA_PARAMETERS MAP {'TYPE': 'postgres', 'SECRET': 'postgres'}
      );
      -- DuckLake attachment with Postgres catalog
      ATTACH 'ducklake:pg_secret' AS my_ducklake;
    # Optional: Specify which topics to include (if not specified, all topics are included)
    # topics: ["events", "people"]
    # Reconciliation settings for ensuring S3 and DuckLake are synchronized
    reconciliation:
      on_startup: true        # Run reconciliation when application starts
      interval_minutes: 60    # Check for missing files every hour
      max_concurrent_registrations: 10  # Parallel registration limit

# Rate Limiting Configuration
rate_limiting:
  disabled: false
  max_requests: 15000000
  burst_limit: 20000000
  global_limit: 150000000
  base_size_bytes: 4096

# TLS Configuration
tls:
  disabled: true  # Disabled for development
  # cert_path: "/path/to/cert.pem"
  # key_path: "/path/to/key.pem"
  # cert_pem: "-----BEGIN CERTIFICATE-----\n..."
  # key_pem: "-----BEGIN PRIVATE KEY-----\n..."
  # grpc_default_ssl_roots_file_path: "/path/to/ca-certificates.crt"

# Authentication Configuration
auth:
  providers: []  # Empty for development - no auth
  authorization_enabled: false
  admin_groups: []
  read_only_groups: []
  cognito:
    # user_pool_id: "us-east-1_example"
    # region: "us-east-1"
    # audience: "client-id"
  azure:
    # tenant_id: "tenant-id"
    # client_id: "client-id"
    allow_multi_tenant: false
  gcp:
    # client_id: "client-id"
    # project_id: "project-id"
    require_workspace_domain: false
  auth0:
    # tenant: "your-tenant.auth0.com"
    # audience: "your-api-identifier"
    # groups_namespace: "https://your-app.com/groups"
  okta:
    # org_domain: "your-org.okta.com"
    # audience: "api://your-audience"
    # auth_server_id: "your-auth-server"

# Metrics Configuration
metrics:
  port: 8081
  flush_interval_ms: 1000

# Logging Configuration
logging:
  rust_log: "info"